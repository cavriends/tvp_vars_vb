{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K9VZK9peLGR"
   },
   "outputs": [],
   "source": [
    "#General libraries\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Own code\n",
    "sys.path.append('../')\n",
    "from utils.data_utils import create_data, create_dgp_data, transformation, standardize\n",
    "\n",
    "# Suppress scientific notation in numpy\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5iYbb9PekuA"
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('../data/fred_qd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for train and validation set\n",
    "train = 150\n",
    "validate = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJhlqLZCeyQ3"
   },
   "outputs": [],
   "source": [
    "transform=True\n",
    "\n",
    "gdp = transformation(ds[\"GDPC1\"].iloc[2:].to_numpy(), 5, transform)\n",
    "cpi = transformation(ds[\"CPIAUCSL\"].iloc[2:].to_numpy(), 6, transform)\n",
    "fedfund = transformation(ds[\"FEDFUNDS\"].iloc[2:].to_numpy(), 2, transform)\n",
    "\n",
    "series_total = [gdp, cpi, fedfund]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKmb_g9Rfc2S"
   },
   "outputs": [],
   "source": [
    "standardized_series = standardize(series_total, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFLp3M_QB5wh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences_lstm(series, lag):\n",
    "\n",
    "    series = np.array(series)\n",
    "    T = series.shape[1]\n",
    "    M = series.shape[0]\n",
    "\n",
    "    X = torch.Tensor(np.zeros((T, lag, M)))\n",
    "    y = torch.Tensor(np.zeros((T, M)))\n",
    "\n",
    "    sequenced_data = []\n",
    "\n",
    "    for t in range(lag, T):\n",
    "\n",
    "        X[t,:,:] = torch.Tensor(series[:,(t-lag):t]).T\n",
    "        y[t,:] = torch.Tensor(series[:,t])\n",
    "\n",
    "        sequenced_data.append((X[t,:,:], y[t,:]))\n",
    "\n",
    "    return X, y, sequenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0KBCQQQGwHh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class VanillaLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=3, hidden_size=36, output_size=3, seq_length=4, num_layers=1, dropout=0):\n",
    "\n",
    "        super(VanillaLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_output = (torch.randn(num_layers,1,hidden_size),\n",
    "                              torch.randn(num_layers,1,hidden_size))\n",
    "    \n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden_output = (torch.randn(self.num_layers,1,self.hidden_size),\n",
    "                              torch.randn(self.num_layers,1,self.hidden_size))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, self.hidden_output = self.lstm(sequence.view(len(sequence), 1, -1), self.hidden_output)\n",
    "        prediction = self.linear(lstm_out)\n",
    "\n",
    "        return prediction[-1]\n",
    "    \n",
    "    def train(self, train_data, test_data, loss_f, optimizer_f):\n",
    " \n",
    "        accumulated_loss_test = None\n",
    "\n",
    "        for seq, y in train_data:\n",
    "            \n",
    "            accumulated_loss_train = 0\n",
    "            \n",
    "            self.zero_grad()\n",
    "            self.reset_hidden_state()\n",
    "            \n",
    "            y_pred = self.forward(seq)\n",
    "            loss_train = loss_f(y_pred.squeeze(), y)\n",
    "            accumulated_loss_train += loss_train.item()\n",
    "            \n",
    "            loss_train.backward()\n",
    "            optimizer_f.step()\n",
    "            \n",
    "        if test_data != None: \n",
    "\n",
    "            for seq, y in test_data:\n",
    "\n",
    "                accumulated_loss_test = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    y_pred = self.forward(seq)\n",
    "                    loss_test = loss_f(y_pred.squeeze(), y)\n",
    "                    accumulated_loss_test += loss_test.item()\n",
    "                \n",
    "        return accumulated_loss_train, accumulated_loss_test\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \n",
    "        accumulated_loss = 0\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for seq, y in data:\n",
    "\n",
    "                y_pred = self.forward(seq).squeeze()\n",
    "                accumulated_loss += ((y_pred - y)**2).numpy()\n",
    "                predictions.append(y_pred.numpy())\n",
    "\n",
    "        return predictions, accumulated_loss/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "91EvlNmVQxIo",
    "outputId": "17cb022f-5d7b-48a9-81b9-ab5121dbea54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possibilities: 48\n",
      "Progress: 3/48\n",
      "Progress: 6/48\n",
      "Progress: 9/48\n",
      "Progress: 12/48\n",
      "Progress: 15/48\n",
      "Progress: 18/48\n",
      "Progress: 21/48\n",
      "Progress: 24/48\n",
      "Progress: 27/48\n",
      "Progress: 30/48\n",
      "Progress: 33/48\n",
      "Progress: 36/48\n",
      "Progress: 39/48\n",
      "Progress: 42/48\n",
      "Progress: 45/48\n",
      "Progress: 48/48\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# Generate parameter space for (discrete) grid search \n",
    "hidden_size = np.arange(12,36,2)\n",
    "lag_length = np.arange(4,12,2)\n",
    "parameter_sets = []\n",
    "optimal_states = []\n",
    "print_status = False\n",
    "loss_list = []\n",
    "\n",
    "for h in hidden_size:\n",
    "    for l in lag_length:\n",
    "        parameter_sets.append((h, l))\n",
    "\n",
    "        \n",
    "# Number of variables is 3 (or M)\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 200\n",
    "\n",
    "number_of_possibilities = len(parameter_sets)u8 \n",
    "print(f'Number of possibilities: {number_of_possibilities}')\n",
    "\n",
    "# Grid search over parameters, best validation loss determines the optimal state for the set of parameters\n",
    "for idx, p_set in enumerate(parameter_sets):\n",
    "\n",
    "    h_size, lag_length = p_set\n",
    "    vanilla_lstm = VanillaLSTM(input_size, \n",
    "                             h_size, \n",
    "                             output_size,\n",
    "                             seq_length=lag_length)\n",
    "\n",
    "    loss_f = nn.MSELoss()\n",
    "    optimizer_f = optim.Adam(vanilla_lstm.parameters(), lr=1e-3)\n",
    "\n",
    "    X, y, sequenced_data = create_sequences_lstm(standardized_series, lag_length)\n",
    "\n",
    "    train_sequenced = sequenced_data[:train]\n",
    "    validate_sequenced = sequenced_data[train:validate]\n",
    "\n",
    "    losses = [] \n",
    "    lstm_states = []\n",
    "    \n",
    "    begin_time = time.time()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        train_loss, validation_loss = vanilla_lstm.train(train_sequenced, validate_sequenced, loss_f, optimizer_f)\n",
    "        losses.append(validation_loss)\n",
    "\n",
    "        lstm_states.append(vanilla_lstm.state_dict())\n",
    "\n",
    "        if print_status:\n",
    "            if i % (epochs//4) == 0:\n",
    "                \n",
    "                elapsed_time = time.time() - begin_time\n",
    "                begin_time = time.time()\n",
    "                \n",
    "                progress_string = (f'Epoch: {i} - h: {h_size} & lag: {lag_length} \\n'\n",
    "                                   f' Train loss: \\t  {np.round(train_loss,5)} \\n'\n",
    "                                   f' Validation loss: {np.round(validation_loss,5)} \\n'\n",
    "                                   f' Elapsed time: \\t  {np.round(elapsed_time,5)} seconds')\n",
    "                \n",
    "                print(progress_string)\n",
    "\n",
    "    if print_status:\n",
    "        clear_output()\n",
    "    \n",
    "    # Determine the lowes validation loss and save the state of the LSTM\n",
    "    optimal_loss = losses.index(min(losses))\n",
    "    optimal_states.append(lstm_states[optimal_loss])\n",
    "    \n",
    "    # Add lowest validation loss to list\n",
    "    loss_list.append(losses[optimal_loss])\n",
    "    \n",
    "    # Keep an eye on progress\n",
    "    if (idx+1)%(number_of_possibilities//16) == 0:\n",
    "\n",
    "        print(f'Progress: {idx+1}/{number_of_possibilities}')\n",
    "\n",
    "    if (idx+1) == number_of_possibilities:\n",
    "        \n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUsxUs41AA5y"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Dump results from grid search to disk\n",
    "dump_to_disk = (optimal_states, loss_list)\n",
    "\n",
    "with open(\"dumps/training.pkl\", 'wb') as f:\n",
    "    \n",
    "    pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open grid search dump from disk\n",
    "\n",
    "# with open('training.pkl', 'rb') as f:\n",
    "\n",
    "#     dump_from_disk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find optimal parameters and its state\n",
    "optimal_parameters_index = loss_list.index(min(loss_list))\n",
    "\n",
    "optimal_state = optimal_states[optimal_parameters_index]\n",
    "optimal_parameters = parameter_sets[optimal_parameters_index]\n",
    "\n",
    "X, y, sequenced_data = create_sequences_lstm(standardized_series, lag_length)\n",
    "\n",
    "# Complete training set\n",
    "train_sequenced = sequenced_data[:validate]\n",
    "\n",
    "optimal_lstm = VanillaLSTM(input_size=input_size, hidden_size=optimal_parameters[0], output_size=output_size, seq_length=optimal_parameters[1])\n",
    "\n",
    "# Load optimal state\n",
    "optimal_lstm.load_state_dict(optimal_state)\n",
    "\n",
    "# Train the optimal LSTM on the complete training set for 25 (a guess) epochs\n",
    "epochs = 25\n",
    "\n",
    "lstm_states = []\n",
    "losses = []\n",
    "\n",
    "loss_f = nn.MSELoss()\n",
    "optimizer_f = optim.Adam(optimal_lstm.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_loss, __ = optimal_lstm.train(train_sequenced, None, loss_f, optimizer_f)\n",
    "    losses.append(train_loss)\n",
    "    lstm_states.append(optimal_lstm.state_dict())\n",
    "\n",
    "# Load the best state from training on complete dataset (based on lowest training loss)\n",
    "optimal_lstm.load_state_dict(lstm_states[losses.index(min(losses))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vanilla LSTM with hidden size: 20 & sequence length: 4 | OOS MSE: 0.17357532680034637\n"
     ]
    }
   ],
   "source": [
    "# Calculate OOS MSE\n",
    "predictions, loss = optimal_lstm.predict(sequenced_data[validate:])\n",
    "\n",
    "print(f' vanilla LSTM with hidden size: {optimal_parameters[0]} & sequence length: {optimal_parameters[1]} | OOS MSE: {loss.mean()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-TVP-VAR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
