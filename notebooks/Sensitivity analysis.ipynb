{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis\n",
    "\n",
    "The sensitivity analysis is carried out in this notebook. An extra note of caution is that this notebook also runs completely parallel using pool.starmap(). This uses up all the resources available on the system. Hence, don't be afraid when your laptop flies to the moon or freezes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy import io\n",
    "import pickle\n",
    "\n",
    "# Own code\n",
    "sys.path.append(\"../\")\n",
    "from utils.data_utils import generate_contemp_matrices, transformation, standardize\n",
    "from utils.tvp_models import TVPVARModel, tvp_ar_contemp_decomposition\n",
    "\n",
    "# Suppress scientific notation in numpy\n",
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set M and standardization\n",
    "\n",
    "M = 3\n",
    "standardization = False\n",
    "transform = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"../data/fred_qd.csv\")\n",
    "gdp = transformation(ds[\"GDPC1\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "cpi = transformation(ds[\"CPIAUCSL\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "fedfund = transformation(ds[\"FEDFUNDS\"].iloc[2:].to_numpy(), 2, transform, scale=1)\n",
    "compi = transformation(ds[\"PPIACO\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "borrowings = transformation(ds[\"TOTRESNS\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "sp500 = transformation(ds[\"S&P 500\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "m2 = transformation(ds[\"M2REAL\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "\n",
    "# Start due to transformation\n",
    "\n",
    "lag = 1\n",
    "\n",
    "if M == 3:\n",
    "\n",
    "    series = [gdp[lag:], cpi[lag:], fedfund[lag:]]\n",
    "    \n",
    "elif M == 7:\n",
    "    \n",
    "    series = [gdp[lag:], cpi[lag:], fedfund[lag:],compi[lag:],borrowings[lag:],sp500[lag:],m2[lag:]]\n",
    "    \n",
    "\n",
    "if standardization:\n",
    "    \n",
    "    series = standardize(series, train = 243-25)\n",
    "\n",
    "series_total = np.array(series)\n",
    "\n",
    "y_matrix_contemp, X_matrix_contemp = generate_contemp_matrices(244, M, 1, series_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"lasso_alternative\"\n",
    "train = T - 25\n",
    "\n",
    "model = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=100)\n",
    "msfe, *_, msfe_complete, alpl_complete = model.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $a_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_lasso_a0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"lasso_alternative\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "\n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0_lasso\":parameter_value+error,\"b0_lasso\":1e-3}\n",
    "    prior_parameters_minus = {\"a0_lasso\":parameter_value-error,\"b0_lasso\":1e-3}\n",
    "    \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "       \n",
    "    msfe_complete_plus_reshaped = msfe_complete_plus.reshape(8,M,25).mean(2).mean(0)\n",
    "    msfe_complete_minus_reshaped = msfe_complete_minus.reshape(8,M,25).mean(2).mean(0)\n",
    "        \n",
    "    derivative_complete = (msfe_complete_plus_reshaped - msfe_complete_minus_reshaped)/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 6 -> Derivative: [-8.94810720e-08  4.96198181e-09  9.85780386e-08]\n",
      "Run: 11 -> Derivative: [-8.51806878e-08  5.62357939e-09  9.69240447e-08]\n",
      "Run: 7 -> Derivative: [-8.88194744e-08  5.29278060e-09  9.79164411e-08]\n",
      "Run: 4 -> Derivative: [-9.17966635e-08  4.96198181e-09  9.95704350e-08]\n",
      "Run: 5 -> Derivative: [-9.04734684e-08  4.96198181e-09  9.92396362e-08]\n",
      "Run: 2 -> Derivative: [-9.37814562e-08  4.79658242e-09  9.99012338e-08]\n",
      "Run: 15 -> Derivative: [-8.20380993e-08  6.28517696e-09  9.65932459e-08]\n",
      "Run: 3 -> Derivative: [-9.27890599e-08  4.96198181e-09  1.00232033e-07]\n",
      "Run: 9 -> Derivative: [-8.68346817e-08  5.29278060e-09  9.82472399e-08]\n",
      "Run: 14 -> Derivative: [-8.28650962e-08  5.78897878e-09  9.56008496e-08]\n",
      "Run: 12 -> Derivative: [-8.41882914e-08  5.95437817e-09  9.75856423e-08]\n",
      "Run: 8 -> Derivative: [-8.76616787e-08  5.29278060e-09  9.75856423e-08]\n",
      "Run: 13 -> Derivative: [-8.36920932e-08  6.28517696e-09  9.49392520e-08]\n",
      "Run: 1 -> Derivative: [-9.46084532e-08  5.12738120e-09  1.00562831e-07]\n",
      "Run: 10 -> Derivative: [-8.58422853e-08  5.45817999e-09  9.75856423e-08]\n",
      "Run: 16 -> Derivative: [-8.10457029e-08  5.78897878e-09  9.42776544e-08]\n",
      "Run: 17 -> Derivative: [-8.00533066e-08  5.62357939e-09  9.49392520e-08]\n",
      "Run: 19 -> Derivative: [-7.87301114e-08  5.95437817e-09  9.39468556e-08]\n",
      "Run: 20 -> Derivative: [-7.77377150e-08  6.45057635e-09  9.39468556e-08]\n",
      "Run: 18 -> Derivative: [-7.93917090e-08  5.78897878e-09  9.32852580e-08]\n",
      "Run: 21 -> Derivative: [-7.74069163e-08  6.45057635e-09  9.36160568e-08]\n",
      "Run: 24 -> Derivative: [-7.49259253e-08  6.28517696e-09  9.19620629e-08]\n",
      "Run: 26 -> Derivative: [-7.32719314e-08  6.78137514e-09  9.09696665e-08]\n",
      "Run: 25 -> Derivative: [-7.42643278e-08  6.45057635e-09  9.09696665e-08]\n",
      "Run: 22 -> Derivative: [-7.65799193e-08  6.45057635e-09  9.32852580e-08]\n",
      "Run: 27 -> Derivative: [-7.32719314e-08  6.28517696e-09  9.06388677e-08]\n",
      "Run: 23 -> Derivative: [-7.57529223e-08  6.61597575e-09  9.19620629e-08]\n",
      "Run: 29 -> Derivative: [-7.12871387e-08  6.11977757e-09  8.93156726e-08]\n",
      "Run: 31 -> Derivative: [-7.04601417e-08  6.61597575e-09  8.83232762e-08]\n",
      "Run: 32 -> Derivative: [-6.97985441e-08  6.94677454e-09  8.86540750e-08]\n",
      "Run: 28 -> Derivative: [-7.21141357e-08  6.28517696e-09  8.96464714e-08]\n",
      "Run: 30 -> Derivative: [-7.07909405e-08  6.78137514e-09  8.89848738e-08]\n",
      "Run: 33 -> Derivative: [-6.89715472e-08  6.78137514e-09  8.83232762e-08]\n",
      "Run: 34 -> Derivative: [-6.83099496e-08  6.45057635e-09  8.73308799e-08]\n",
      "Run: 35 -> Derivative: [-6.78137514e-08  6.78137514e-09  8.70000811e-08]\n",
      "Run: 36 -> Derivative: [-6.73175532e-08  7.11217393e-09  8.66692823e-08]\n",
      "Run: 37 -> Derivative: [-6.68213551e-08  6.45057635e-09  8.53460872e-08]\n",
      "Run: 40 -> Derivative: [-6.50019617e-08  6.61597575e-09  8.43536908e-08]\n",
      "Run: 45 -> Derivative: [-6.21901720e-08  7.11217393e-09  8.40228920e-08]\n",
      "Run: 38 -> Derivative: [-6.59943581e-08  6.78137514e-09  8.66692823e-08]\n",
      "Run: 39 -> Derivative: [-6.53327605e-08  6.78137514e-09  8.56768859e-08]\n",
      "Run: 44 -> Derivative: [-6.25209708e-08  6.94677454e-09  8.36920932e-08]\n",
      "Run: 43 -> Derivative: [-6.31825684e-08  7.27757332e-09  8.36920932e-08]\n",
      "Run: 42 -> Derivative: [-6.38441660e-08  6.94677454e-09  8.40228920e-08]\n",
      "Run: 41 -> Derivative: [-6.43403642e-08  6.94677454e-09  8.46844896e-08]\n",
      "Run: 47 -> Derivative: [-6.10323763e-08  7.11217393e-09  8.26996969e-08]\n",
      "Run: 46 -> Derivative: [-6.15285745e-08  6.94677454e-09  8.26996969e-08]\n",
      "Run: 48 -> Derivative: [-6.07015775e-08  6.94677454e-09  8.26996969e-08]\n",
      "Run: 49 -> Derivative: [-5.98745805e-08  7.11217393e-09  8.13765017e-08]\n",
      "Run: 51 -> Derivative: [-5.92129829e-08  6.61597575e-09  8.07149041e-08]\n",
      "Run: 52 -> Derivative: [-5.83859860e-08  7.11217393e-09  8.00533066e-08]\n",
      "Run: 50 -> Derivative: [-5.93783823e-08  6.78137514e-09  8.13765017e-08]\n",
      "Run: 53 -> Derivative: [-5.82205866e-08  7.11217393e-09  7.87301114e-08]\n",
      "Run: 54 -> Derivative: [-5.75589890e-08  6.78137514e-09  7.93917090e-08]\n",
      "Run: 55 -> Derivative: [-5.72281902e-08  6.61597575e-09  7.83993126e-08]\n",
      "Run: 56 -> Derivative: [-5.65665926e-08  6.78137514e-09  7.87301114e-08]\n",
      "Run: 59 -> Derivative: [-5.54087969e-08  6.94677454e-09  7.67453187e-08]\n",
      "Run: 57 -> Derivative: [-5.60703945e-08  6.94677454e-09  7.80685138e-08]\n",
      "Run: 58 -> Derivative: [-5.57395957e-08  6.78137514e-09  7.70761175e-08]\n",
      "Run: 60 -> Derivative: [-5.49125987e-08  6.78137514e-09  7.70761175e-08]\n",
      "Run: 61 -> Derivative: [-5.45817999e-08  6.61597575e-09  7.57529223e-08]\n",
      "Run: 62 -> Derivative: [-5.40856017e-08  6.94677454e-09  7.70761175e-08]\n",
      "Run: 63 -> Derivative: [-5.34240042e-08  7.60837211e-09  7.54221235e-08]\n",
      "Run: 64 -> Derivative: [-5.29278060e-08  6.78137514e-09  7.54221235e-08]\n",
      "CPU times: user 4.72 s, sys: 1.23 s, total: 5.95 s\n",
      "Wall time: 1h 24min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1e-3\n",
    "finish = 2\n",
    "interval = 64\n",
    "\n",
    "a0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_lasso_a0, a0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"lasso_alternative\"\n",
    "parameter = \"a0_lasso\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_M_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $b_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_lasso_b0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"lasso_alternative\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 75\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0_lasso\":1e-3,\"b0_lasso\":parameter_value+error}\n",
    "    prior_parameters_minus = {\"a0_lasso\":1e-3,\"b0_lasso\":parameter_value-error}\n",
    "        \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "\n",
    "    derivative_complete = (msfe_complete_plus.mean(0) - msfe_complete_minus.mean(0))/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1e-3\n",
    "finish = 1\n",
    "interval = 256\n",
    "\n",
    "b0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_lasso_b0, b0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,3], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"lasso_alternative\"\n",
    "parameter = \"b0_lasso\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_vb_based_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"svss\"\n",
    "\n",
    "train = T - 25\n",
    "\n",
    "prior_parameters = {\"g0\":1,\"h0\":12,\"pi0\":0.5}\n",
    "\n",
    "model = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=100, prior_parameters=prior_parameters)\n",
    "msfe, *_  = model.result()\n",
    "\n",
    "# msfe, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=75, prior_parameters=prior_paramters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $g_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_svss_g0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"svss\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 75\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"g0\":parameter_value+error,\"h0\":1e-2, \"pi0\":0.5}\n",
    "    prior_parameters_minus = {\"g0\":parameter_value-error,\"h0\":1e-2, \"pi0\":0.5}\n",
    "    \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "    \n",
    "    derivative_complete = (msfe_complete_plus.mean(0) - msfe_complete_minus.mean(0))/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1\n",
    "finish = 1.5\n",
    "interval = 64\n",
    "\n",
    "g0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_svss_g0, g0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,75], k=3)  # type: BSpline+\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"svss\"\n",
    "parameter = \"g0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_std_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $h_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_svss_h0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"svss\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 75\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"g0\":1,\"h0\":parameter_value+error, \"pi0\":0.5}\n",
    "    prior_parameters_minus = {\"g0\":1,\"h0\":parameter_value-error, \"pi0\":0.5}\n",
    "    \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "    \n",
    "    derivative_complete = (msfe_complete_plus.mean(0) - msfe_complete_minus.mean(0))/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 12\n",
    "finish = 14\n",
    "interval = 128\n",
    "\n",
    "h0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_svss_h0, h0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4)\n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,75], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"svss\"\n",
    "parameter = \"h0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"horseshoe\"\n",
    "train = T - 25\n",
    "\n",
    "prior_parameters = {\"a0\":5, \"b0\":1}\n",
    "\n",
    "msfe, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=100, prior_parameters=prior_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $a_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_horseshoe_a0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"horseshoe\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 150\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0\":parameter_value+error,\"b0\":1}\n",
    "    prior_parameters_minus = {\"a0\":parameter_value-error,\"b0\":1}\n",
    "    \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "    \n",
    "    derivative_complete = (msfe_complete_plus.mean(0) - msfe_complete_minus.mean(0))/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 4\n",
    "finish = 5\n",
    "interval = 256\n",
    "\n",
    "a0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_horseshoe_a0, a0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"horseshoe\"\n",
    "parameter = \"a0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_vb_based_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $b_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_horseshoe_b0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"horseshoe\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0\":4,\"b0\":parameter_value+error}\n",
    "    prior_parameters_minus = {\"a0\":4,\"b0\":parameter_value-error}\n",
    "    \n",
    "    model_plus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_plus, *_, msfe_complete_plus, alpl_complete_plus = model_plus.result()\n",
    "    \n",
    "    model_minus = tvp_ar_contemp_decomposition(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    msfe_minus, *_, msfe_complete_minus, alpl_complete_minus = model_minus.result()\n",
    "    \n",
    "    derivative_complete = (msfe_complete_plus.mean(0) - msfe_complete_minus.mean(0))/(2*error)\n",
    "    derivative_msfe = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    derivative_h_step = (msfe_plus - msfe_minus)/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative_complete}')\n",
    "    \n",
    "    return [parameter_value, derivative_complete, derivative_msfe, derivative_h_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1\n",
    "finish = 1.5\n",
    "interval = 256\n",
    "\n",
    "b0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_horseshoe_b0, b0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), interval*4) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,7], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"horseshoe\"\n",
    "parameter = \"b0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}_vb_based_huber.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
