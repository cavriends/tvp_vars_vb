{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy import io\n",
    "import pickle\n",
    "\n",
    "# Own code\n",
    "sys.path.append(\"../\")\n",
    "from utils.data_utils import generate_contemp_matrices, transformation, standardize\n",
    "from utils.tvp_models import TVPVARModel, tvp_ar_contemp, tvp_ar_non_contemp\n",
    "\n",
    "# Suppress scientific notation in numpy\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set M and standardization\n",
    "\n",
    "M = 3\n",
    "standardization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = True\n",
    "\n",
    "ds = pd.read_csv(\"../data/fred_qd.csv\")\n",
    "gdp = transformation(ds[\"GDPC1\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "cpi = transformation(ds[\"CPIAUCSL\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "fedfund = transformation(ds[\"FEDFUNDS\"].iloc[2:].to_numpy(), 2, transform, scale=1)\n",
    "compi = transformation(ds[\"PPIACO\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "borrowings = transformation(ds[\"TOTRESNS\"].iloc[2:].to_numpy(), 6, transform, scale=1)\n",
    "sp500 = transformation(ds[\"S&P 500\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "m2 = transformation(ds[\"M2REAL\"].iloc[2:].to_numpy(), 5, transform, scale=1)\n",
    "\n",
    "# Start due to transformation\n",
    "\n",
    "lag = 1\n",
    "\n",
    "if M == 3:\n",
    "\n",
    "    series = [gdp[lag:], cpi[lag:], fedfund[lag:]]\n",
    "    \n",
    "elif M == 7:\n",
    "    \n",
    "    series = [gdp[lag:], cpi[lag:], fedfund[lag:],compi[lag:],borrowings[lag:],sp500[lag:],m2[lag:]]\n",
    "    \n",
    "\n",
    "if standardized:\n",
    "    \n",
    "    series = standardize(series, train = 243-25)\n",
    "\n",
    "series_total = np.array(series)\n",
    "\n",
    "y_matrix_contemp, X_matrix_contemp = generate_contemp_matrices(244, M, 1, series_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"lasso_alternative\"\n",
    "train = T - 25\n",
    "\n",
    "msfe, alpl, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $a_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_lasso_a0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"lasso_alternative\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "\n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0_lasso\":parameter_value+error,\"b0_lasso\":1e-3}\n",
    "    prior_parameters_minus = {\"a0_lasso\":parameter_value-error,\"b0_lasso\":1e-3}\n",
    "    \n",
    "    msfe_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1e-3\n",
    "finish = 2\n",
    "interval = 64\n",
    "\n",
    "a0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_lasso_a0, a0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 256) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"lasso_alternative\"\n",
    "parameter = \"a0_lasso\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $b_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_lasso_b0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"lasso_alternative\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0_lasso\":1e-3,\"b0_lasso\":parameter_value+error}\n",
    "    prior_parameters_minus = {\"a0_lasso\":1e-3,\"b0_lasso\":parameter_value-error}\n",
    "    \n",
    "    msfe_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1e-3\n",
    "finish = 1e-2\n",
    "interval = 64\n",
    "\n",
    "b0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_lasso_b0, b0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 528) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"lasso_alternative\"\n",
    "parameter = \"b0_lasso\"\n",
    "start = 1e-3\n",
    "finish = 1e-2\n",
    "interval = 64\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 \n",
      " Elapsed time: 8.2302 seconds\n",
      "Seconds for one iteration: 0.8002\n",
      " Difference: 0.0547\n",
      "Seconds for one iteration: 0.8074\n",
      " Difference: 0.0412\n",
      "Seconds for one iteration: 0.7795\n",
      " Difference: 0.0162\n",
      "Seconds for one iteration: 0.832\n",
      " Difference: 0.0109\n",
      "Seconds for one iteration: 0.8555\n",
      " Difference: 0.0048\n",
      "Seconds for one iteration: 0.8665\n",
      " Difference: 0.0023\n",
      "Seconds for one iteration: 0.7183\n",
      " Difference: 0.0011\n",
      "Seconds for one iteration: 0.7692\n",
      " Difference: 0.0007\n",
      "Seconds for one iteration: 0.8017\n",
      " Difference: 0.0003\n",
      "Seconds for one iteration: 0.7935\n",
      " Difference: 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6e22684e4866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprior_paramters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"g0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"h0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pi0\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmsfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvp_ar_contemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_matrix_contemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_matrix_contemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior_paramters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/ESE/MSc Econometrics/Thesis/Bayesian VARs/Code/Jupyter/utils/tvp_models.py\u001b[0m in \u001b[0;36mtvp_ar_contemp\u001b[0;34m(T, M, p, train, X, y, prior, total_h, iterations, print_status, prior_parameters)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0msigma_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvp_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mmsfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvp_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mmsfe_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsfe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/ESE/MSc Econometrics/Thesis/Bayesian VARs/Code/Jupyter/utils/tvp_models.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(self, total_h, number_of_draws, constant, print_status)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0malpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_oos_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_draws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/ESE/MSc Econometrics/Thesis/Bayesian VARs/Code/Jupyter/utils/tvp_models.py\u001b[0m in \u001b[0;36mcalculate_oos_predictions\u001b[0;34m(self, total_h, constant, number_of_draws, print_status)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mprev_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/ESE/MSc Econometrics/Thesis/Bayesian VARs/Code/Jupyter/utils/tvp_models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, threshold, print_status, kl_bound)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mFtilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmt1t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmt1t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmtt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSt1t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSt1t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mStt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"svss\"\n",
    "\n",
    "train = T - 25\n",
    "\n",
    "prior_paramters = {\"g0\":1e-2,\"h0\":1e-2,\"pi0\":0.5}\n",
    "\n",
    "msfe, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=50, prior_parameters=prior_paramters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $g_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_svss_g0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"svss\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"g0\":parameter_value+error,\"h0\":1e-2, \"pi0\":0.5}\n",
    "    prior_parameters_minus = {\"g0\":parameter_value-error,\"h0\":1e-2, \"pi0\":0.5}\n",
    "    \n",
    "    msfe_plus, alpl_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, apl_min, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 1\n",
    "finish = 2\n",
    "interval = 128\n",
    "\n",
    "g0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_svss_g0, g0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 528) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline+\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"svss\"\n",
    "parameter = \"g0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $h_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_svss_h0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"svss\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"g0\":1e-2,\"h0\":parameter_value+error, \"pi0\":0.5}\n",
    "    prior_parameters_minus = {\"g0\":1e-2,\"h0\":parameter_value-error, \"pi0\":0.5}\n",
    "    \n",
    "    msfe_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 12\n",
    "finish = 20\n",
    "interval = 64\n",
    "\n",
    "h0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_svss_h0, h0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 528) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"svss\"\n",
    "parameter = \"h0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model, given the specific data, converges at all\n",
    "\n",
    "T = 243\n",
    "p = 1\n",
    "prior = \"horseshoe\"\n",
    "train = T - 25\n",
    "\n",
    "msfe, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=True, iterations=500,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $a_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_horseshoe_a0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"horseshoe\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0\":parameter_value+error,\"b0\":1}\n",
    "    prior_parameters_minus = {\"a0\":parameter_value-error,\"b0\":1}\n",
    "    \n",
    "    msfe_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 10\n",
    "finish = 20\n",
    "interval = 64\n",
    "\n",
    "a0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_horseshoe_a0, a0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 528) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"horseshoe\"\n",
    "parameter = \"a0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter -> $b_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative_horseshoe_b0(iteration, parameter_value):\n",
    "    \n",
    "    prior = \"horseshoe\"\n",
    "    T = 243\n",
    "    p = 1\n",
    "    train = T-25\n",
    "    iterations = 100\n",
    "    \n",
    "    error = np.sqrt(1.1e-16)\n",
    "    prior_parameters_plus = {\"a0\":1/2,\"b0\":parameter_value+error}\n",
    "    prior_parameters_minus = {\"a0\":1/2,\"b0\":parameter_value-error}\n",
    "    \n",
    "    msfe_plus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_plus)\n",
    "    msfe_minus, *_ = tvp_ar_contemp(T, M, p, train, X_matrix_contemp, y_matrix_contemp, prior, print_status=False, iterations=iterations, prior_parameters=prior_parameters_minus)\n",
    "    \n",
    "    derivative = (msfe_plus.mean() - msfe_minus.mean())/(2*error)\n",
    "    \n",
    "    print(f'Run: {iteration+1} -> Derivative: {derivative}')\n",
    "    \n",
    "    return [parameter_value, derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool, Array\n",
    "\n",
    "# They are going to be some disgusting warnings for the first iterations of the model (has to do with initialization)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(12345)\n",
    "\n",
    "start = 10\n",
    "finish = 11\n",
    "interval = 64\n",
    "\n",
    "b0_set = [(i,value) for i, value in enumerate(np.linspace(start, finish, interval))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pool = Pool()\n",
    "    derivatives = pool.starmap(calculate_derivative_horseshoe_b0, b0_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Visualise the result\n",
    "\n",
    "result = np.block(derivatives)\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(result[:,0].min(), result[:,0].max(), 528) \n",
    "\n",
    "spl = make_interp_spline(result[:,0], result[:,1], k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = \"horseshoe\"\n",
    "parameter = \"b0\"\n",
    "\n",
    "dump_to_disk = [derivatives, [xnew, power_smooth]]\n",
    "\n",
    "with open(f'../sensitivity/results_{M}_{prior}_{parameter}_{start}_{finish}_{interval}.pkl\"', 'wb') as f:\n",
    "        pickle.dump(dump_to_disk, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
